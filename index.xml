<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jingjing Xu on Jingjing Xu</title>
    <link>https://jingjingxupku.github.io/</link>
    <description>Recent content in Jingjing Xu on Jingjing Xu</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Mar 2019 01:03:50 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>My Article Name1</title>
      <link>https://jingjingxupku.github.io/post/my-article-name1/</link>
      <pubDate>Sun, 31 Mar 2019 01:03:50 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/post/my-article-name1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Review-Driven Multi-Label Music Style Classification by Exploiting Style Correlations</title>
      <link>https://jingjingxupku.github.io/publication/review/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/review/</guid>
      <description>&lt;p&gt;This paper explores a new natural language processing task, review-driven multi-label music style classification. This task requires systems to identify multiple styles of music based on its reviews on websites. The biggest challenge lies in the complicated relations of music styles.  To tackle this problem, we propose a novel deep learning approach to automatically learn and exploit style correlations. Experiment results show that our approach achieves large improvements over baselines on the proposed dataset. Furthermore, the visualized analysis shows that our approach performs well in capturing style correlations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Article Name</title>
      <link>https://jingjingxupku.github.io/post/my-article-name/</link>
      <pubDate>Sat, 30 Mar 2019 23:05:51 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/post/my-article-name/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method</title>
      <link>https://jingjingxupku.github.io/publication/my_pub1/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/my_pub1/</guid>
      <description>&lt;p&gt;We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost. Based on the sparsified gradients, we further simplify the model by eliminating the rows or columns that are seldom updated, which will reduce the computational cost both in the training and decoding, and potentially accelerate decoding in real-world applications. Surprisingly, experimental results demonstrate that most of time we only need to update fewer than 5% of the weights at each back propagation pass. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The model simplification results show that we could adaptively simplify the model which could often be reduced by around 9x, without any loss on accuracy or even with improved accuracy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pkuseg Toolkit</title>
      <link>https://jingjingxupku.github.io/project/my-project-2/</link>
      <pubDate>Mon, 25 Feb 2019 11:04:02 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/project/my-project-2/</guid>
      <description>

&lt;p&gt;A multi-domain Chinese word segmentation toolkit. &lt;a href=&#34;https://github.com/lancopku/pkuseg-python&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;highlights&#34;&gt;Highlights&lt;/h2&gt;

&lt;p&gt;The pkuseg-python toolkit has the following features:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Supporting multi-domain Chinese word segmentation. Pkuseg-python supports multi-domain segmentation, including domains like news, web, medicine, and tourism. Users are free to choose different pre-trained models according to the domain features of the text to be segmented. If not sure the domain of the text, users are recommended to use the default model trained on mixed-domain data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Higher word segmentation results. Compared with existing word segmentation toolkits, pkuseg-python can achieve higher F1 scores on the same dataset.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Supporting model training. Pkuseg-python  also supports users to train a new segmentation model with their own data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Supporting POS tagging. We also provide users POS tagging interfaces for further lexical analysis.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;

&lt;p&gt;Ruixuan Luo,  Jingjing Xu, Xuancheng Ren, Yi Zhang, Bingzhen Wei，Xu Sun&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pkuseg Toolkit</title>
      <link>https://jingjingxupku.github.io/project/my-project-3/</link>
      <pubDate>Mon, 25 Feb 2019 11:04:02 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/project/my-project-3/</guid>
      <description>

&lt;p&gt;Commonsense Question Answering &lt;a href=&#34;https://arxiv.org/abs/1909.05311&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Commonsense question answering aims to answer questions which require background knowledge that is not explicitly expressed in the question. The key challenge is how to obtain evidence from external knowledge and make predictions based on the evidence. Recent works either learn to generate evidence from human-annotated evidence which is expensive to collect, or extract evidence from either structured or unstructured knowledge bases which fails to take advantages of both sources. In this work, we propose to automatically extract evidence from heterogeneous knowledge sources, and answer questions based on the extracted evidence. Specifically, we extract evidence from both structured knowledge base (i.e. ConceptNet) and Wikipedia plain texts. We construct graphs for both sources to obtain the relational structures of evidence. Based on these graphs, we propose a graph-based approach consisting of a graph-based contextual word representation learning module and a graph-based inference module. The first module utilizes graph structural information to re-define the distance between words for learning better contextual word representations. The second module adopts graph convolutional network to encode neighbor information into the representations of nodes, and aggregates evidence with graph attention mechanism for predicting the final answer. Experimental results on CommonsenseQA dataset illustrate that our graph-based approach over both knowledge sources brings improvement over strong baselines. Our approach achieves the state-of-the-art accuracy (75.3%) on the CommonsenseQA leaderboard.&lt;/p&gt;

&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;

&lt;p&gt;Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Songlin Hu&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JD Dialog Challenge</title>
      <link>https://jingjingxupku.github.io/project/my-project-1/</link>
      <pubDate>Wed, 31 Oct 2018 11:04:02 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/project/my-project-1/</guid>
      <description>&lt;p&gt;In October 2018, we participated in JD Dialogue Challenge (JDDC), which is a task-oriented multi-turn dialogue challenge, and won the &lt;strong&gt;championship in the automatic evaluation and the second place in the manual evaluation&lt;/strong&gt;. We also won the &amp;ldquo;Excellent Tutor Award&amp;rdquo; and the &amp;ldquo;Architecture Innovation Award&amp;rdquo;. Participants include Ruixuan Luo, Xuancheng Ren, Junyang Lin, Jingjing Xu, Shu Liu and so on. The instructor is Xu Sun.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation</title>
      <link>https://jingjingxupku.github.io/publication/skelton/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/skelton/</guid>
      <description>&lt;p&gt;Narrative story generation is a challenging
problem because it demands the generated
sentences with tight semantic connections,
which has not been well studied by most existing
generative models. To address this problem,
we propose a skeleton-based model to
promote the coherence of generated stories.
Different from traditional models that generate
a complete sentence at a stroke, the proposed
model first generates the most critical phrases,
called skeleton, and then expands the skeleton
to a complete and fluent sentence. The skeleton
is not manually defined, but learned by a
reinforcement learning method. Compared to
the state-of-the-art models, our skeleton-based
model can generate significantly more coherent
text according to human evaluation and automatic
evaluation. The G-score is improved
by 20.1% in human evaluation&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DP-GAN: A Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text</title>
      <link>https://jingjingxupku.github.io/publication/dpgan/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/dpgan/</guid>
      <description>&lt;p&gt;Existing text generation methods tend to produce
repeated and &amp;ldquo;boring&amp;rdquo; expressions. To
tackle this problem, we propose a new text
generation model, called Diversity-Promoting
Generative Adversarial Network (DP-GAN).
The proposed model assigns low reward for
repeatedly generated text and high reward
for &amp;ldquo;novel&amp;rdquo; and fluent text, encouraging the
generator to produce diverse and informative
text. Moreover, we propose a novel language model
based discriminator, which can better
distinguish novel text from repeated text without
the saturation problem compared with existing
classifier-based discriminators. The experimental
results on review generation and dialogue
generation tasks demonstrate that our
model can generate substantially more diverse
and informative text than existing baselines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation</title>
      <link>https://jingjingxupku.github.io/publication/aematch/</link>
      <pubDate>Sat, 21 Jul 2018 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/aematch/</guid>
      <description>&lt;p&gt;Generating semantically coherent responses is
still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and
responses in conversations is more complicated, which highly demands the understanding of utterance-level semantic dependency, a
relation between the whole meanings of inputs and outputs. To address this problem, we
propose an Auto-Encoder Matching (AEM)
model to learn such dependency. The model
contains two auto-encoders and one mapping
module. The auto-encoders learn the semantic representations of inputs and responses,
and the mapping module learns to connect the
utterance-level representations. Experimental
results from automatic and human evaluations
demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach</title>
      <link>https://jingjingxupku.github.io/publication/sentiment/</link>
      <pubDate>Fri, 20 Jul 2018 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/sentiment/</guid>
      <description>&lt;p&gt;The goal of sentiment-to-sentiment “translation” is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of
parallel data. To solve this problem, we
propose a cycled reinforcement learning
method that enables training on unpaired
data by collaboration between a neutralization module and an emotionalization
module. We evaluate our approach on two
review datasets, Yelp and Amazon. Experimental results show that our approach significantly outperforms the state-of-the-art
systems. Especially, the proposed method
substantially improves the content preservation performance. The BLEU score is
improved from 1.64 to 22.46 and from
0.56 to 14.06 on the two datasets, respectively.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cross-Domain and Semi-Supervised Named Entity Recognition in Chinese Social Media: A Unified Model</title>
      <link>https://jingjingxupku.github.io/publication/cross/</link>
      <pubDate>Wed, 20 Jun 2018 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/cross/</guid>
      <description>&lt;p&gt;Named entity recognition (NER) in Chinese social media is an important, but challenging task because Chinese social media language is informal and noisy. Most previous methods on NER focus on in-domain supervised learning, which is limited by scarce annotated data in social media. In this paper, we present that sufficient corpora in formal domains and massive unannotated text can be combined to improve the NER performance in social media. We propose a unified model which can learn from out-of-domain corpora and in-domain unannotated text. The unified model is composed of two parts. One is for cross-domain learning and the other is for semisupervised learning. Cross-domain learning can learn out-of-domain information based on domain similarity. Semisupervised learning can learn in-domain unannotated information by self-training. Experimental results show that our unified model yields a 9.57% improvement over strong baselines and achieves the state-of-the-art performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Sentiment Memories for Sentiment Modification without Parallel Data</title>
      <link>https://jingjingxupku.github.io/publication/memory/</link>
      <pubDate>Wed, 20 Jun 2018 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/memory/</guid>
      <description>&lt;p&gt;The task of sentiment modification requires reversing the sentiment of the input and preserving the sentiment-independent content. However, aligned sentences with the same content
but different sentiments are usually unavailable. Due to the lack of such parallel data, it is
hard to extract sentiment independent content
and reverse the sentiment in an unsupervised
way. Previous work usually can not reconcile
sentiment transformation and content preservation. In this paper, motivated by the fact the
non-emotional context (e.g., “staff”) provides
strong cues for the occurrence of emotional
words (e.g., “friendly”), we propose a novel
method that automatically extracts appropriate
sentiment information from learned sentiment
memories according to specific context. Experiments show that our method substantially
improves the content preservation degree and
achieves the state-of-the-art performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization</title>
      <link>https://jingjingxupku.github.io/publication/summrization/</link>
      <pubDate>Tue, 20 Jun 2017 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/summrization/</guid>
      <description>&lt;p&gt;Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transfer Deep Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network</title>
      <link>https://jingjingxupku.github.io/publication/lowresource/</link>
      <pubDate>Sat, 20 May 2017 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/lowresource/</guid>
      <description>&lt;p&gt;Recent studies have shown effectiveness in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. We propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data. Experiment results show that our work significantly improves the performance on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB datasets. Furthermore, this paper achieves state-of-the-art results: 96.1%, and 96.2% F-score on PKU and CTB datasets.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
