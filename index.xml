<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jingjing Xu on Jingjing Xu</title>
    <link>https://jingjingxupku.github.io/</link>
    <description>Recent content in Jingjing Xu on Jingjing Xu</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Sep 2019 00:00:00 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Understanding and Improving Layer Normalization</title>
      <link>https://jingjingxupku.github.io/publication/neurips2019/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/neurips2019/</guid>
      <description>&lt;p&gt;Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm. Many of previous studies believe that the success of LayerNorm comes from forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Asking Clarification Questions in Knowledge-Based Question Answering</title>
      <link>https://jingjingxupku.github.io/publication/emnlpclarification/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/emnlpclarification/</guid>
      <description>&lt;p&gt;The ability to ask clarification questions is essential for knowledge-based question answering (KBQA) systems, especially for handling ambiguous phenomena. Despite its importance, clarification has not been well explored in current KBQA systems. Further progress requires supervised resources for training and evaluation, and powerful models for clarification-related text understanding and generation. In this paper, we construct a new clarification dataset, CLAQUA, with nearly 40K open-domain examples. The dataset supports three serial tasks: given a question, identify whether clarification is needed; if yes, generate a clarification question; then predict answers base on external user feedback. We provide representative baselines for these tasks and further introduce a coarse-to-fine model for clarification question generation. Experiments show that the proposed model achieves better performance than strong baselines. The further analysis demonstrates that our dataset brings new challenges and there still remain several unsolved problems, like reasonable automatic evaluation metrics for clarification question generation and powerful models for handling entity sparsity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LexicalAT: Lexical-Based Adversarial Reinforcement Training for Robust Sentiment Classification</title>
      <link>https://jingjingxupku.github.io/publication/emnlplexical/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/emnlplexical/</guid>
      <description>&lt;p&gt;Recent work has shown that current text classification models are fragile and sensitive to simple perturbations. In this work, we propose a novel adversarial training approach, LexicalAT, to improve the robustness of current classification models. The proposed approach consists of a generator and a classifier. The generator learns to generate examples to attack the classifier while the classifier learns to defend these attacks. Considering the diversity of attacks, the generator uses a large-scale lexical knowledge base, WordNet, to generate attacking examples by replacing some words in training examples with their synonyms (e.g., sad and unhappy), neighbor words (e.g., fox and wolf), or super-superior words (e.g., chair and armchair). Due to the discrete generation step in the generator, we use policy gradient, a reinforcement learning approach, to train the two modules. Experiments show LexicalAT outperforms strong baselines and reduces test errors on various neural networks, including CNN, RNN, and BERT.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coherent Comments Generation for Chinese Articles with a Graph-to-Sequence Model</title>
      <link>https://jingjingxupku.github.io/publication/acl2019/</link>
      <pubDate>Sun, 30 Jun 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/acl2019/</guid>
      <description>&lt;p&gt;Automatic article commenting is helpful in encouraging user engagement and interaction on online news platforms. However, the news documents are usually too long for traditional encoder-decoder based models, which often results in general and irrelevant comments. In this paper, we propose to generate comments with a graph-to-sequence model that models the input news as a topic interaction graph. By organizing the article into graph structure, our model can better understand the internal structure of the article and the connection between topics, which makes it better able to understand the story. We collect and release a large scale news-comment corpus from a popular Chinese online news platform Tencent Kuaibao. Extensive experiment results show that our model can generate much more coherent and informative comments compared with several strong baseline models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Article Name1</title>
      <link>https://jingjingxupku.github.io/post/my-article-name1/</link>
      <pubDate>Sun, 31 Mar 2019 01:03:50 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/post/my-article-name1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Review-Driven Multi-Label Music Style Classification by Exploiting Style Correlations</title>
      <link>https://jingjingxupku.github.io/publication/review/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/review/</guid>
      <description>&lt;p&gt;This paper explores a new natural language processing task, review-driven multi-label music style classification. This task requires systems to identify multiple styles of music based on its reviews on websites. The biggest challenge lies in the complicated relations of music styles.  To tackle this problem, we propose a novel deep learning approach to automatically learn and exploit style correlations. Experiment results show that our approach achieves large improvements over baselines on the proposed dataset. Furthermore, the visualized analysis shows that our approach performs well in capturing style correlations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Article Name</title>
      <link>https://jingjingxupku.github.io/post/my-article-name/</link>
      <pubDate>Sat, 30 Mar 2019 23:05:51 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/post/my-article-name/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method</title>
      <link>https://jingjingxupku.github.io/publication/my_pub1/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/my_pub1/</guid>
      <description>&lt;p&gt;We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost. Based on the sparsified gradients, we further simplify the model by eliminating the rows or columns that are seldom updated, which will reduce the computational cost both in the training and decoding, and potentially accelerate decoding in real-world applications. Surprisingly, experimental results demonstrate that most of time we only need to update fewer than 5% of the weights at each back propagation pass. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The model simplification results show that we could adaptively simplify the model which could often be reduced by around 9x, without any loss on accuracy or even with improved accuracy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Commonsense Question Answering</title>
      <link>https://jingjingxupku.github.io/project/my-project-3/</link>
      <pubDate>Mon, 25 Feb 2019 11:04:02 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/project/my-project-3/</guid>
      <description>

&lt;p&gt;Commonsense Question Answering &lt;a href=&#34;https://arxiv.org/abs/1909.05311&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Commonsense question answering aims to answer questions which require background knowledge that is not explicitly expressed in the question. The key challenge is how to obtain evidence from external knowledge and make predictions based on the evidence. Recent works either learn to generate evidence from human-annotated evidence which is expensive to collect, or extract evidence from either structured or unstructured knowledge bases which fails to take advantages of both sources. In this work, we propose to automatically extract evidence from heterogeneous knowledge sources, and answer questions based on the extracted evidence. Specifically, we extract evidence from both structured knowledge base (i.e. ConceptNet) and Wikipedia plain texts. We construct graphs for both sources to obtain the relational structures of evidence. Based on these graphs, we propose a graph-based approach consisting of a graph-based contextual word representation learning module and a graph-based inference module. The first module utilizes graph structural information to re-define the distance between words for learning better contextual word representations. The second module adopts graph convolutional network to encode neighbor information into the representations of nodes, and aggregates evidence with graph attention mechanism for predicting the final answer. Experimental results on CommonsenseQA dataset illustrate that our graph-based approach over both knowledge sources brings improvement over strong baselines. Our approach achieves the state-of-the-art accuracy (75.3%) on the CommonsenseQA leaderboard.&lt;/p&gt;

&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;

&lt;p&gt;Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Songlin Hu&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pkuseg Toolkit</title>
      <link>https://jingjingxupku.github.io/project/my-project-2/</link>
      <pubDate>Mon, 25 Feb 2019 11:04:02 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/project/my-project-2/</guid>
      <description>

&lt;p&gt;A multi-domain Chinese word segmentation toolkit. &lt;a href=&#34;https://github.com/lancopku/pkuseg-python&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;highlights&#34;&gt;Highlights&lt;/h2&gt;

&lt;p&gt;The pkuseg-python toolkit has the following features:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Supporting multi-domain Chinese word segmentation. Pkuseg-python supports multi-domain segmentation, including domains like news, web, medicine, and tourism. Users are free to choose different pre-trained models according to the domain features of the text to be segmented. If not sure the domain of the text, users are recommended to use the default model trained on mixed-domain data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Higher word segmentation results. Compared with existing word segmentation toolkits, pkuseg-python can achieve higher F1 scores on the same dataset.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Supporting model training. Pkuseg-python  also supports users to train a new segmentation model with their own data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Supporting POS tagging. We also provide users POS tagging interfaces for further lexical analysis.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;

&lt;p&gt;Ruixuan Luo,  Jingjing Xu, Xuancheng Ren, Yi Zhang, Bingzhen Wei，Xu Sun&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JD Dialog Challenge</title>
      <link>https://jingjingxupku.github.io/project/my-project-1/</link>
      <pubDate>Wed, 31 Oct 2018 11:04:02 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/project/my-project-1/</guid>
      <description>&lt;p&gt;In October 2018, we participated in JD Dialogue Challenge (JDDC), which is a task-oriented multi-turn dialogue challenge, and won the &lt;strong&gt;championship in the automatic evaluation and the second place in the manual evaluation&lt;/strong&gt;. We also won the &amp;ldquo;Excellent Tutor Award&amp;rdquo; and the &amp;ldquo;Architecture Innovation Award&amp;rdquo;. Participants include Ruixuan Luo, Xuancheng Ren, Junyang Lin, Jingjing Xu, Shu Liu and so on. The instructor is Xu Sun.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation</title>
      <link>https://jingjingxupku.github.io/publication/skelton/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/skelton/</guid>
      <description>&lt;p&gt;Narrative story generation is a challenging
problem because it demands the generated
sentences with tight semantic connections,
which has not been well studied by most existing
generative models. To address this problem,
we propose a skeleton-based model to
promote the coherence of generated stories.
Different from traditional models that generate
a complete sentence at a stroke, the proposed
model first generates the most critical phrases,
called skeleton, and then expands the skeleton
to a complete and fluent sentence. The skeleton
is not manually defined, but learned by a
reinforcement learning method. Compared to
the state-of-the-art models, our skeleton-based
model can generate significantly more coherent
text according to human evaluation and automatic
evaluation. The G-score is improved
by 20.1% in human evaluation&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DP-GAN: A Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text</title>
      <link>https://jingjingxupku.github.io/publication/dpgan/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/dpgan/</guid>
      <description>&lt;p&gt;Existing text generation methods tend to produce
repeated and &amp;ldquo;boring&amp;rdquo; expressions. To
tackle this problem, we propose a new text
generation model, called Diversity-Promoting
Generative Adversarial Network (DP-GAN).
The proposed model assigns low reward for
repeatedly generated text and high reward
for &amp;ldquo;novel&amp;rdquo; and fluent text, encouraging the
generator to produce diverse and informative
text. Moreover, we propose a novel language model
based discriminator, which can better
distinguish novel text from repeated text without
the saturation problem compared with existing
classifier-based discriminators. The experimental
results on review generation and dialogue
generation tasks demonstrate that our
model can generate substantially more diverse
and informative text than existing baselines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation</title>
      <link>https://jingjingxupku.github.io/publication/aematch/</link>
      <pubDate>Sat, 21 Jul 2018 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/aematch/</guid>
      <description>&lt;p&gt;Generating semantically coherent responses is
still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and
responses in conversations is more complicated, which highly demands the understanding of utterance-level semantic dependency, a
relation between the whole meanings of inputs and outputs. To address this problem, we
propose an Auto-Encoder Matching (AEM)
model to learn such dependency. The model
contains two auto-encoders and one mapping
module. The auto-encoders learn the semantic representations of inputs and responses,
and the mapping module learns to connect the
utterance-level representations. Experimental
results from automatic and human evaluations
demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach</title>
      <link>https://jingjingxupku.github.io/publication/sentiment/</link>
      <pubDate>Fri, 20 Jul 2018 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/sentiment/</guid>
      <description>&lt;p&gt;The goal of sentiment-to-sentiment “translation” is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of
parallel data. To solve this problem, we
propose a cycled reinforcement learning
method that enables training on unpaired
data by collaboration between a neutralization module and an emotionalization
module. We evaluate our approach on two
review datasets, Yelp and Amazon. Experimental results show that our approach significantly outperforms the state-of-the-art
systems. Especially, the proposed method
substantially improves the content preservation performance. The BLEU score is
improved from 1.64 to 22.46 and from
0.56 to 14.06 on the two datasets, respectively.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
