<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wei Li on Jingjing Xu</title>
    <link>https://jingjingxupku.github.io/authors/wei-li/</link>
    <description>Recent content in Wei Li on Jingjing Xu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 30 Jun 2019 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://jingjingxupku.github.io/authors/wei-li/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Coherent Comments Generation for Chinese Articles with a Graph-to-Sequence Model</title>
      <link>https://jingjingxupku.github.io/publication/acl2019/</link>
      <pubDate>Sun, 30 Jun 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/acl2019/</guid>
      <description>Automatic article commenting is helpful in encouraging user engagement and interaction on online news platforms. However, the news documents are usually too long for traditional encoder-decoder based models, which often results in general and irrelevant comments. In this paper, we propose to generate comments with a graph-to-sequence model that models the input news as a topic interaction graph. By organizing the article into graph structure, our model can better understand the internal structure of the article and the connection between topics, which makes it better able to understand the story.</description>
    </item>
    
    <item>
      <title>Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method</title>
      <link>https://jingjingxupku.github.io/publication/my_pub1/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/my_pub1/</guid>
      <description>We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost.</description>
    </item>
    
  </channel>
</rss>