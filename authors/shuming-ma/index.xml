<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shuming Ma on Jingjing Xu</title>
    <link>https://jingjingxupku.github.io/authors/shuming-ma/</link>
    <description>Recent content in Shuming Ma on Jingjing Xu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 30 Mar 2019 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://jingjingxupku.github.io/authors/shuming-ma/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method</title>
      <link>https://jingjingxupku.github.io/publication/my_pub1/</link>
      <pubDate>Sat, 30 Mar 2019 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/my_pub1/</guid>
      <description>We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost.</description>
    </item>
    
    <item>
      <title>Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization</title>
      <link>https://jingjingxupku.github.io/publication/summrization/</link>
      <pubDate>Tue, 20 Jun 2017 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/summrization/</guid>
      <description>Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder.</description>
    </item>
    
    <item>
      <title>Transfer Deep Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network</title>
      <link>https://jingjingxupku.github.io/publication/lowresource/</link>
      <pubDate>Sat, 20 May 2017 00:00:00 +0800</pubDate>
      
      <guid>https://jingjingxupku.github.io/publication/lowresource/</guid>
      <description>Recent studies have shown effectiveness in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. We propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data.</description>
    </item>
    
  </channel>
</rss>