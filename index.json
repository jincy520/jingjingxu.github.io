[{"authors":["admin"],"categories":null,"content":" I am a PhD candidate, supervised by Prof. Xu Sun, at MOE Key Laboratory of Computational Linguistics, School of Electronics Engineering and Computer Science, Peking University. I received my Bachelor degree from Northwest A\u0026amp;F University in 2015.\nI have great interests in Natural Language Processing and Deep Learning. Currently, my research areas include knowledge-aware language understanding and generation, adversarial attack for robust machine learning.\nNews  2019-12-23: Looking to adopt a cat\n 2019-11-11: 1 paper accepted by AAAI 2020\n 2019-09-25: Got 5-4 Scholarship（the highest-level scholarship of Peking University）\n 2019-09-04: 1 paper accepted by NeurIPS 2019\n 2019-08-13: 2 papers accepted by EMNLP 2019\n 2019-06-20: Got President Scholarship\n 2019-06-10: Got Top-10 Award (the highest-level award of EECS)\n 2019-02-22: 1 paper accepted by NAACL 2019\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://jingjingxupku.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am a PhD candidate, supervised by Prof. Xu Sun, at MOE Key Laboratory of Computational Linguistics, School of Electronics Engineering and Computer Science, Peking University. I received my Bachelor degree from Northwest A\u0026amp;F University in 2015.\nI have great interests in Natural Language Processing and Deep Learning. Currently, my research areas include knowledge-aware language understanding and generation, adversarial attack for robust machine learning.\nNews  2019-12-23: Looking to adopt a cat","tags":null,"title":"Jingjing Xu","type":"author"},{"authors":["Jingjing Xu","Xu Sun","Zhiyuan Zhang","Guangxiang Zhao","Junyang Lin"],"categories":null,"content":"Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm. Many of previous studies believe that the success of LayerNorm comes from forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets.\n","date":1569772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569772800,"objectID":"26803e27027bc1376e181ecf35d6dc80","permalink":"https://jingjingxupku.github.io/publication/neurips2019/","publishdate":"2019-09-30T00:00:00+08:00","relpermalink":"/publication/neurips2019/","section":"publication","summary":"NeurIPS 2019","tags":[],"title":"Understanding and Improving Layer Normalization","type":"publication"},{"authors":["Jingjing Xu","Yuechen Wang","Duyu Tang","Nan Duan","Pengcheng Yang","Qi Zeng","Ming Zhou","Xu Sun"],"categories":null,"content":"The ability to ask clarification questions is essential for knowledge-based question answering (KBQA) systems, especially for handling ambiguous phenomena. Despite its importance, clarification has not been well explored in current KBQA systems. Further progress requires supervised resources for training and evaluation, and powerful models for clarification-related text understanding and generation. In this paper, we construct a new clarification dataset, CLAQUA, with nearly 40K open-domain examples. The dataset supports three serial tasks: given a question, identify whether clarification is needed; if yes, generate a clarification question; then predict answers base on external user feedback. We provide representative baselines for these tasks and further introduce a coarse-to-fine model for clarification question generation. Experiments show that the proposed model achieves better performance than strong baselines. The further analysis demonstrates that our dataset brings new challenges and there still remain several unsolved problems, like reasonable automatic evaluation metrics for clarification question generation and powerful models for handling entity sparsity.\n","date":1564502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564502400,"objectID":"5f55320d69e81d6660bbe2e8889a547c","permalink":"https://jingjingxupku.github.io/publication/emnlpclarification/","publishdate":"2019-07-31T00:00:00+08:00","relpermalink":"/publication/emnlpclarification/","section":"publication","summary":"EMNLP 2019","tags":[],"title":"Asking Clarification Questions in Knowledge-Based Question Answering","type":"publication"},{"authors":["Jingjing Xu","Liang Zhao","Hanqi Yan","Qi Zeng","Yun Liang","Xu Sun"],"categories":null,"content":"Recent work has shown that current text classification models are fragile and sensitive to simple perturbations. In this work, we propose a novel adversarial training approach, LexicalAT, to improve the robustness of current classification models. The proposed approach consists of a generator and a classifier. The generator learns to generate examples to attack the classifier while the classifier learns to defend these attacks. Considering the diversity of attacks, the generator uses a large-scale lexical knowledge base, WordNet, to generate attacking examples by replacing some words in training examples with their synonyms (e.g., sad and unhappy), neighbor words (e.g., fox and wolf), or super-superior words (e.g., chair and armchair). Due to the discrete generation step in the generator, we use policy gradient, a reinforcement learning approach, to train the two modules. Experiments show LexicalAT outperforms strong baselines and reduces test errors on various neural networks, including CNN, RNN, and BERT.\n","date":1564502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564502400,"objectID":"a8fa3968724e93446ffd18c9755a2b74","permalink":"https://jingjingxupku.github.io/publication/emnlplexical/","publishdate":"2019-07-31T00:00:00+08:00","relpermalink":"/publication/emnlplexical/","section":"publication","summary":"EMNLP 2019","tags":[],"title":"LexicalAT: Lexical-Based Adversarial Reinforcement Training for Robust Sentiment Classification","type":"publication"},{"authors":["Wei Li","**Jingjing Xu**","Yancheng He","Shengli Yan","Yunfang Wu","Xu sun"],"categories":null,"content":"Automatic article commenting is helpful in encouraging user engagement and interaction on online news platforms. However, the news documents are usually too long for traditional encoder-decoder based models, which often results in general and irrelevant comments. In this paper, we propose to generate comments with a graph-to-sequence model that models the input news as a topic interaction graph. By organizing the article into graph structure, our model can better understand the internal structure of the article and the connection between topics, which makes it better able to understand the story. We collect and release a large scale news-comment corpus from a popular Chinese online news platform Tencent Kuaibao. Extensive experiment results show that our model can generate much more coherent and informative comments compared with several strong baseline models.\n","date":1561824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561824000,"objectID":"34b10369e84a59f1879af2a0c97f2b57","permalink":"https://jingjingxupku.github.io/publication/acl2019/","publishdate":"2019-06-30T00:00:00+08:00","relpermalink":"/publication/acl2019/","section":"publication","summary":"ACL 2019","tags":[],"title":"Coherent Comments Generation for Chinese Articles with a Graph-to-Sequence Model","type":"publication"},{"authors":[],"categories":[],"content":"","date":1553965430,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553965430,"objectID":"21bc7503bdc3d92c369b82469933aaf3","permalink":"https://jingjingxupku.github.io/post/my-article-name1/","publishdate":"2019-03-31T01:03:50+08:00","relpermalink":"/post/my-article-name1/","section":"post","summary":"","tags":[],"title":"My Article Name1","type":"post"},{"authors":["Guangxiang Zhao*","**JingjingXu***","Qi Zeng","Xuancheng Ren","Xu Sun"],"categories":null,"content":"This paper explores a new natural language processing task, review-driven multi-label music style classification. This task requires systems to identify multiple styles of music based on its reviews on websites. The biggest challenge lies in the complicated relations of music styles. To tackle this problem, we propose a novel deep learning approach to automatically learn and exploit style correlations. Experiment results show that our approach achieves large improvements over baselines on the proposed dataset. Furthermore, the visualized analysis shows that our approach performs well in capturing style correlations.\n","date":1553961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553961600,"objectID":"4cfb42ec4880d229b170da04c3da7674","permalink":"https://jingjingxupku.github.io/publication/review/","publishdate":"2019-03-31T00:00:00+08:00","relpermalink":"/publication/review/","section":"publication","summary":"NAACL 2019","tags":[],"title":"Review-Driven Multi-Label Music Style Classification by Exploiting Style Correlations","type":"publication"},{"authors":[],"categories":[],"content":"","date":1553958351,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553958351,"objectID":"2550a6b6b1e3ef22f904f2d225585395","permalink":"https://jingjingxupku.github.io/post/my-article-name/","publishdate":"2019-03-30T23:05:51+08:00","relpermalink":"/post/my-article-name/","section":"post","summary":"","tags":[],"title":"My Article Name","type":"post"},{"authors":["Xu Sun*","Xuancheng Ren*","Shuming Ma","Bingzhen Wei","Wei Li","**Jingjing Xu**"],"categories":null,"content":"We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost. Based on the sparsified gradients, we further simplify the model by eliminating the rows or columns that are seldom updated, which will reduce the computational cost both in the training and decoding, and potentially accelerate decoding in real-world applications. Surprisingly, experimental results demonstrate that most of time we only need to update fewer than 5% of the weights at each back propagation pass. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The model simplification results show that we could adaptively simplify the model which could often be reduced by around 9x, without any loss on accuracy or even with improved accuracy.\n","date":1553875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553875200,"objectID":"6b7e46c53a7f9d30a1e3bbd7e2a1f983","permalink":"https://jingjingxupku.github.io/publication/my_pub1/","publishdate":"2019-03-30T00:00:00+08:00","relpermalink":"/publication/my_pub1/","section":"publication","summary":"TKDE 2019","tags":[],"title":"Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method","type":"publication"},{"authors":null,"categories":null,"content":" Commonsense Question Answering link\nCommonsense question answering aims to answer questions which require background knowledge that is not explicitly expressed in the question. The key challenge is how to obtain evidence from external knowledge and make predictions based on the evidence. Recent works either learn to generate evidence from human-annotated evidence which is expensive to collect, or extract evidence from either structured or unstructured knowledge bases which fails to take advantages of both sources. In this work, we propose to automatically extract evidence from heterogeneous knowledge sources, and answer questions based on the extracted evidence. Specifically, we extract evidence from both structured knowledge base (i.e. ConceptNet) and Wikipedia plain texts. We construct graphs for both sources to obtain the relational structures of evidence. Based on these graphs, we propose a graph-based approach consisting of a graph-based contextual word representation learning module and a graph-based inference module. The first module utilizes graph structural information to re-define the distance between words for learning better contextual word representations. The second module adopts graph convolutional network to encode neighbor information into the representations of nodes, and aggregates evidence with graph attention mechanism for predicting the final answer. Experimental results on CommonsenseQA dataset illustrate that our graph-based approach over both knowledge sources brings improvement over strong baselines. Our approach achieves the state-of-the-art accuracy (75.3%) on the CommonsenseQA leaderboard.\nAuthors Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Songlin Hu\n","date":1551063842,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551063842,"objectID":"ec5eb54e8ebe256de568f1d9ec798541","permalink":"https://jingjingxupku.github.io/project/my-project-3/","publishdate":"2019-02-25T11:04:02+08:00","relpermalink":"/project/my-project-3/","section":"project","summary":"I have participated in a commonsense question answering project in MSRA. Our team kept the first in leaderboard until now. I am responsible for knowledge extraction and a part of reasoning code implementation.","tags":[],"title":"Commonsense Question Answering","type":"project"},{"authors":null,"categories":null,"content":" A multi-domain Chinese word segmentation toolkit. link\nHighlights The pkuseg-python toolkit has the following features:\n Supporting multi-domain Chinese word segmentation. Pkuseg-python supports multi-domain segmentation, including domains like news, web, medicine, and tourism. Users are free to choose different pre-trained models according to the domain features of the text to be segmented. If not sure the domain of the text, users are recommended to use the default model trained on mixed-domain data.\n Higher word segmentation results. Compared with existing word segmentation toolkits, pkuseg-python can achieve higher F1 scores on the same dataset.\n Supporting model training. Pkuseg-python also supports users to train a new segmentation model with their own data.\n Supporting POS tagging. We also provide users POS tagging interfaces for further lexical analysis.\n  Authors Ruixuan Luo, Jingjing Xu, Xuancheng Ren, Yi Zhang, Bingzhen Wei，Xu Sun\n","date":1551063842,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551063842,"objectID":"5026b947f355f1c8b5b6cdb2b2b59d6b","permalink":"https://jingjingxupku.github.io/project/my-project-2/","publishdate":"2019-02-25T11:04:02+08:00","relpermalink":"/project/my-project-2/","section":"project","summary":"We designed a multi-domain Chinese word segmentation toolkit, called pkuseg, supporting domains like news, web, medicine, and tourism. This work has got **4K stars** on github until now.","tags":[],"title":"Pkuseg Toolkit","type":"project"},{"authors":null,"categories":null,"content":"In October 2018, we participated in JD Dialogue Challenge (JDDC), which is a task-oriented multi-turn dialogue challenge, and won the championship in the automatic evaluation and the second place in the manual evaluation. We also won the \u0026ldquo;Excellent Tutor Award\u0026rdquo; and the \u0026ldquo;Architecture Innovation Award\u0026rdquo;. Participants include Ruixuan Luo, Xuancheng Ren, Junyang Lin, Jingjing Xu, Shu Liu and so on. The instructor is Xu Sun.\n","date":1540955042,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540955042,"objectID":"2fee882dddfb5124b4d37262e426d66a","permalink":"https://jingjingxupku.github.io/project/my-project-1/","publishdate":"2018-10-31T11:04:02+08:00","relpermalink":"/project/my-project-1/","section":"project","summary":"In October 2018, we participated in JD Dialogue Challenge (JDDC) and won the **championship in the automatic evaluation and the second place in the manual evaluation**.","tags":[],"title":"JD Dialog Challenge","type":"project"},{"authors":["**Jingjing Xu**","Yi Zhang","Qi Zeng","Xuancheng Ren","Xiaoyan Cai","Xu Sun"],"categories":null,"content":"Narrative story generation is a challenging problem because it demands the generated sentences with tight semantic connections, which has not been well studied by most existing generative models. To address this problem, we propose a skeleton-based model to promote the coherence of generated stories. Different from traditional models that generate a complete sentence at a stroke, the proposed model first generates the most critical phrases, called skeleton, and then expands the skeleton to a complete and fluent sentence. The skeleton is not manually defined, but learned by a reinforcement learning method. Compared to the state-of-the-art models, our skeleton-based model can generate significantly more coherent text according to human evaluation and automatic evaluation. The G-score is improved by 20.1% in human evaluation\n","date":1533052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533052800,"objectID":"c5c9f8caf9c1cdbf82f1b5d637c4ced8","permalink":"https://jingjingxupku.github.io/publication/skelton/","publishdate":"2018-08-01T00:00:00+08:00","relpermalink":"/publication/skelton/","section":"publication","summary":"EMNLP 2018","tags":[],"title":"A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation","type":"publication"},{"authors":["**Jingjing Xu**","Xuancheng Ren","Junyang Lin","Xu Sun"],"categories":null,"content":"Existing text generation methods tend to produce repeated and \u0026ldquo;boring\u0026rdquo; expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The proposed model assigns low reward for repeatedly generated text and high reward for \u0026ldquo;novel\u0026rdquo; and fluent text, encouraging the generator to produce diverse and informative text. Moreover, we propose a novel language model based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines.\n","date":1533052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533052800,"objectID":"dc57ae1664d530864052043df9b064b0","permalink":"https://jingjingxupku.github.io/publication/dpgan/","publishdate":"2018-08-01T00:00:00+08:00","relpermalink":"/publication/dpgan/","section":"publication","summary":"EMNLP 2018","tags":[],"title":"DP-GAN: A Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text","type":"publication"},{"authors":["Liangchen Luo*","**Jingjing Xu***","Junyang Lin","Qi Zeng","Xu Sun"],"categories":null,"content":"Generating semantically coherent responses is still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and responses in conversations is more complicated, which highly demands the understanding of utterance-level semantic dependency, a relation between the whole meanings of inputs and outputs. To address this problem, we propose an Auto-Encoder Matching (AEM) model to learn such dependency. The model contains two auto-encoders and one mapping module. The auto-encoders learn the semantic representations of inputs and responses, and the mapping module learns to connect the utterance-level representations. Experimental results from automatic and human evaluations demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models.\n","date":1532102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532102400,"objectID":"c8ee17330b10dcbe0476dfdf99a19966","permalink":"https://jingjingxupku.github.io/publication/aematch/","publishdate":"2018-07-21T00:00:00+08:00","relpermalink":"/publication/aematch/","section":"publication","summary":"EMNLP 2018","tags":[],"title":"An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation","type":"publication"},{"authors":["**Jingjing Xu***","Xu Sun*","Qi Zeng","Xiaodong Zhang","Xuancheng Ren","Houfeng Wang","Wenjie Li"],"categories":null,"content":"The goal of sentiment-to-sentiment “translation” is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of parallel data. To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module. We evaluate our approach on two review datasets, Yelp and Amazon. Experimental results show that our approach significantly outperforms the state-of-the-art systems. Especially, the proposed method substantially improves the content preservation performance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.\n","date":1532016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532016000,"objectID":"cef8f5857c89c167c11c3a3d3ad0a5bc","permalink":"https://jingjingxupku.github.io/publication/sentiment/","publishdate":"2018-07-20T00:00:00+08:00","relpermalink":"/publication/sentiment/","section":"publication","summary":"ACL 2018","tags":[],"title":"Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach","type":"publication"},{"authors":["**Jingjing Xu**","Hangfeng He","Xu Sun","Xuancheng Ren","Sujian Li"],"categories":null,"content":"Named entity recognition (NER) in Chinese social media is an important, but challenging task because Chinese social media language is informal and noisy. Most previous methods on NER focus on in-domain supervised learning, which is limited by scarce annotated data in social media. In this paper, we present that sufficient corpora in formal domains and massive unannotated text can be combined to improve the NER performance in social media. We propose a unified model which can learn from out-of-domain corpora and in-domain unannotated text. The unified model is composed of two parts. One is for cross-domain learning and the other is for semisupervised learning. Cross-domain learning can learn out-of-domain information based on domain similarity. Semisupervised learning can learn in-domain unannotated information by self-training. Experimental results show that our unified model yields a 9.57% improvement over strong baselines and achieves the state-of-the-art performance.\n","date":1529424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529424000,"objectID":"62f90522fbb793f5842bb90e8013c020","permalink":"https://jingjingxupku.github.io/publication/cross/","publishdate":"2018-06-20T00:00:00+08:00","relpermalink":"/publication/cross/","section":"publication","summary":"TASLP 2018","tags":[],"title":"Cross-Domain and Semi-Supervised Named Entity Recognition in Chinese Social Media: A Unified Model","type":"publication"},{"authors":["Yi Zhang","**Jingjing Xu**","Pengcheng Yang","Xu Sun"],"categories":null,"content":"The task of sentiment modification requires reversing the sentiment of the input and preserving the sentiment-independent content. However, aligned sentences with the same content but different sentiments are usually unavailable. Due to the lack of such parallel data, it is hard to extract sentiment independent content and reverse the sentiment in an unsupervised way. Previous work usually can not reconcile sentiment transformation and content preservation. In this paper, motivated by the fact the non-emotional context (e.g., “staff”) provides strong cues for the occurrence of emotional words (e.g., “friendly”), we propose a novel method that automatically extracts appropriate sentiment information from learned sentiment memories according to specific context. Experiments show that our method substantially improves the content preservation degree and achieves the state-of-the-art performance.\n","date":1529424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529424000,"objectID":"b9d659b89f121b589f42d997252ff5d6","permalink":"https://jingjingxupku.github.io/publication/memory/","publishdate":"2018-06-20T00:00:00+08:00","relpermalink":"/publication/memory/","section":"publication","summary":"EMNLP 2018","tags":[],"title":"Learning Sentiment Memories for Sentiment Modification without Parallel Data","type":"publication"},{"authors":["Shuming Ma","Xu Sun","**Jingjing Xu**","Houfeng Wang","Wenjie Li","Qi Su"],"categories":null,"content":"Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.\n","date":1497888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497888000,"objectID":"cb51307ab809242c354b9a1aebe127cd","permalink":"https://jingjingxupku.github.io/publication/summrization/","publishdate":"2017-06-20T00:00:00+08:00","relpermalink":"/publication/summrization/","section":"publication","summary":"ACL 2017","tags":[],"title":"Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization","type":"publication"},{"authors":["**Jingjing Xu**","Shuming Ma","Yi Zhang","Bingzhen Wei","Xiaoyan Cai","Xu Sun"],"categories":null,"content":"Recent studies have shown effectiveness in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. We propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data. Experiment results show that our work significantly improves the performance on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB datasets. Furthermore, this paper achieves state-of-the-art results: 96.1%, and 96.2% F-score on PKU and CTB datasets.\n","date":1495209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495209600,"objectID":"a7431dbc1d8688e16bd5e118a110f13c","permalink":"https://jingjingxupku.github.io/publication/lowresource/","publishdate":"2017-05-20T00:00:00+08:00","relpermalink":"/publication/lowresource/","section":"publication","summary":"NLPCC 2017","tags":[],"title":"Transfer Deep Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network","type":"publication"},{"authors":["**Jingjing Xu**","Ji Wen","Xu Sun","Qi Su"],"categories":null,"content":"Named Entity Recognition and Relation Extraction for Chinese literature text is regarded as the highly difficult problem, partially because of the lack of tagging sets. In this paper, we build a discourse-level dataset from hundreds of Chinese literature articles for improving this task. To build a high quality dataset, we propose two tagging methods to solve the problem of data inconsistency, including a heuristic tagging method and a machine auxiliary tagging method. Based on this corpus, we also introduce several widely used models to conduct experiments. Experimental results not only show the usefulness of the proposed dataset, but also provide baselines for further research.\n","date":1492617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492617600,"objectID":"47ff99242308467dc7395ec6433fd23d","permalink":"https://jingjingxupku.github.io/publication/discource/","publishdate":"2017-04-20T00:00:00+08:00","relpermalink":"/publication/discource/","section":"publication","summary":"Arxiv 2017","tags":[],"title":"A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text","type":"publication"},{"authors":["**Jingjing Xu**","Xu Sun"],"categories":null,"content":"Recently, many neural network models have been applied to Chinese word segmentation. However, such models focus more on collecting local information while long distance dependencies are not well learned. To integrate local features with long distance dependencies, we propose a dependency-based gated recursive neural network. Local features are first collected by bi-directional long short term memory network, then combined and refined to long distance dependencies via gated recursive neural network. Experimental results show that our model is a competitive model for Chinese word segmentation.\n","date":1461081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461081600,"objectID":"7d23b326bfee2789f273deac6122ddb9","permalink":"https://jingjingxupku.github.io/publication/first/","publishdate":"2016-04-20T00:00:00+08:00","relpermalink":"/publication/first/","section":"publication","summary":"ACL 2016","tags":[],"title":"Dependency-based Gated Recursive Neural Network for Chinese Word Segmentation","type":"publication"}]